{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## BERT Model ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\0860\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\0860\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\0860\\.conda\\envs\\bert\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 200\n",
      "  5%|▌         | 10/200 [00:15<04:58,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3888, 'learning_rate': 2.5e-06, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 20/200 [00:32<05:02,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3985, 'learning_rate': 5e-06, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 30/200 [00:45<03:55,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2575, 'learning_rate': 7.5e-06, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 40/200 [00:59<03:45,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2603, 'learning_rate': 1e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 50/200 [01:13<03:45,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1124, 'learning_rate': 1.25e-05, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 60/200 [01:31<03:52,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9834, 'learning_rate': 1.5e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 70/200 [01:48<03:36,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5583, 'learning_rate': 1.75e-05, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 80/200 [02:04<03:08,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7037, 'learning_rate': 2e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 90/200 [02:19<02:48,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.493, 'learning_rate': 2.25e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 100/200 [02:34<02:29,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1608, 'learning_rate': 2.5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 110/200 [02:49<02:13,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3393, 'learning_rate': 2.7500000000000004e-05, 'epoch': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 120/200 [03:04<01:57,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7075, 'learning_rate': 3e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 130/200 [03:20<01:47,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7962, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 140/200 [03:36<01:35,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4009, 'learning_rate': 3.5e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 150/200 [03:50<01:07,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8175, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 160/200 [04:02<00:46,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5389, 'learning_rate': 4e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 170/200 [04:13<00:33,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.275, 'learning_rate': 4.25e-05, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 180/200 [04:25<00:22,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4409, 'learning_rate': 4.5e-05, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 190/200 [04:36<00:11,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3328, 'learning_rate': 4.75e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [04:50<00:00,  1.51s/it]***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9774, 'learning_rate': 0.0, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 200/200 [04:56<00:00,  1.51s/it]Saving model checkpoint to ./results\\checkpoint-200\n",
      "Configuration saved in ./results\\checkpoint-200\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8584941625595093, 'eval_runtime': 6.5193, 'eval_samples_per_second': 30.678, 'eval_steps_per_second': 7.67, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-200\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results\\checkpoint-200 (score: 0.8584941625595093).\n",
      "100%|██████████| 200/200 [05:02<00:00,  1.51s/it]\n",
      "Configuration saved in bert_crime_model.bin\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 302.2254, 'train_samples_per_second': 2.647, 'train_steps_per_second': 0.662, 'train_loss': 2.247146849632263, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in bert_crime_model.bin\\pytorch_model.bin\n",
      "tokenizer config file saved in bert_crime_tokenizer\\tokenizer_config.json\n",
      "Special tokens file saved in bert_crime_tokenizer\\special_tokens_map.json\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 4\n",
      "100%|██████████| 50/50 [00:06<00:00,  7.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       0.98      1.00      0.99        57\n",
      "           3       0.20      1.00      0.33         1\n",
      "           4       0.79      1.00      0.88        11\n",
      "           5       0.85      1.00      0.92        11\n",
      "           6       0.73      0.79      0.76        24\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       1.00      1.00      1.00        26\n",
      "           9       0.00      0.00      0.00         1\n",
      "          10       0.69      1.00      0.82        27\n",
      "          11       0.00      0.00      0.00         3\n",
      "          12       0.00      0.00      0.00         2\n",
      "          13       0.00      0.00      0.00         4\n",
      "          14       1.00      1.00      1.00         4\n",
      "          15       1.00      1.00      1.00         1\n",
      "          16       0.78      0.78      0.78         9\n",
      "          17       0.00      0.00      0.00         1\n",
      "          18       0.00      0.00      0.00         1\n",
      "          19       0.00      0.00      0.00         3\n",
      "          20       0.00      0.00      0.00         2\n",
      "          21       0.00      0.00      0.00         1\n",
      "          22       0.00      0.00      0.00         1\n",
      "          24       0.00      0.00      0.00         1\n",
      "          28       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.84       200\n",
      "   macro avg       0.38      0.44      0.39       200\n",
      "weighted avg       0.77      0.84      0.80       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\0860\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\0860\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\0860\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import concurrent.futures\n",
    "\n",
    "# Function to load the tokenizer with a timeout\n",
    "def load_tokenizer_with_timeout(model_name, timeout=30):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future = executor.submit(BertTokenizer.from_pretrained, model_name)\n",
    "        try:\n",
    "            tokenizer = future.result(timeout=timeout)\n",
    "        except concurrent.futures.TimeoutError:\n",
    "            print(\"Timeout while loading tokenizer. Skipping.\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error while loading tokenizer: {e}\")\n",
    "            return None\n",
    "    return tokenizer\n",
    "\n",
    "# Function to load the model with a timeout\n",
    "def load_model_with_timeout(model_name, num_labels, timeout=30):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future = executor.submit(BertForSequenceClassification.from_pretrained, model_name, num_labels=num_labels)\n",
    "        try:\n",
    "            model = future.result(timeout=timeout)\n",
    "        except concurrent.futures.TimeoutError:\n",
    "            print(\"Timeout while loading model. Skipping.\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error while loading model: {e}\")\n",
    "            return None\n",
    "    return model\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('Police_Department_Incidents_-_Previous_Year__2016_.csv')\n",
    "\n",
    "# Data Preprocessing\n",
    "data.dropna(subset=['PdDistrict'], inplace=True)  # Drop rows with missing PdDistrict\n",
    "data = data[data['Category'].notna()]  # Drop rows with missing Category\n",
    "\n",
    "\n",
    "data = data.sample(1000)  \n",
    "\n",
    "# Load tokenizer and model with timeout\n",
    "tokenizer = load_tokenizer_with_timeout('bert-base-uncased')\n",
    "if tokenizer is None:\n",
    "    raise RuntimeError(\"Tokenizer could not be loaded. Exiting.\")\n",
    "\n",
    "# Prepare labels\n",
    "label_to_id = {label: i for i, label in enumerate(data['Category'].unique())}\n",
    "data['label_id'] = data['Category'].map(label_to_id)\n",
    "\n",
    "# Prepare text data for BERT\n",
    "def encode_data(texts, labels, max_length=64):  # Reduced max_length for faster processing\n",
    "    encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "    labels = torch.tensor(labels.tolist())\n",
    "    return encodings, labels\n",
    "\n",
    "# Encode the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['Descript'], data['label_id'], test_size=0.2, random_state=42)\n",
    "train_encodings, train_labels = encode_data(X_train, y_train)\n",
    "test_encodings, test_labels = encode_data(X_test, y_test)\n",
    "\n",
    "# Load the model\n",
    "model = load_model_with_timeout('bert-base-uncased', num_labels=len(label_to_id))\n",
    "if model is None:\n",
    "    raise RuntimeError(\"Model could not be loaded. Exiting.\")\n",
    "\n",
    "# Define a dataset class\n",
    "class CrimeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = CrimeDataset(train_encodings, train_labels)\n",
    "test_dataset = CrimeDataset(test_encodings, test_labels)\n",
    "\n",
    "# Training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,  # Reduce the number of epochs\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=200,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",  # Save model at the end of each epoch\n",
    "    save_total_limit=2,  # Save only the last two checkpoints\n",
    "    load_best_model_at_end=True,  # Load the best model when finished training\n",
    "    fp16=False,  # Disable mixed precision for CPU compatibility\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted. Saving partial model...\")\n",
    "    trainer.save_model(\"interrupted_model_save\")\n",
    "    tokenizer.save_pretrained(\"interrupted_tokenizer_save\")\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model_path = 'bert_crime_model.bin'\n",
    "tokenizer_path = 'bert_crime_tokenizer'\n",
    "\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(tokenizer_path)\n",
    "\n",
    "# Prediction and Evaluation\n",
    "preds = trainer.predict(test_dataset)\n",
    "y_pred = torch.argmax(torch.tensor(preds.predictions), axis=1)\n",
    "\n",
    "# Print classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"BERT Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
